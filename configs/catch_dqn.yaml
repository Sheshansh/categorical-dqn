# Catch DQN config file

# env info
label: "Catcher DQN"
env_name: "Catcher-Level0-v0"
env_class: catch

# agent info
agent_type: "dqn"

# experiment settings
seed: 99
cuda: yes
display_plots: no # uses vizdom for plotting

# training vars
training_steps: 1000000
# estimator settings
estimator: catch
batch_size: 64
hidden_size: 128
hist_len: 2
# exploration, q-learning, optimization settings
epsilon: 1
epsilon_steps: 100000
experience_replay: nTupleExperienceReplay
replay_mem_size: 100000
update_freq: 2
target_update_freq: 24
gamma: 0.99
optim: Adam
lr: .000635  # decrease to 0.000135 for hist_len 4
eps: 0.00015  # RMSprop: 0.01 | Adam: 0.01/batch_size

# evaluator
eval_steps: 12000
eval_frequency: 12000
eval_start: 0
eval_env_name: "Catcher-Level0-v0"  # eg.: deterministic or wo skipping frames

# reporting
report_frequency: 12000 # steps
