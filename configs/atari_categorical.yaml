# Example config file

# env info
label: "Atari Categorical"
env_name: "Breakout-v0"
env_class: atari

# agent info
agent_type: "categorical"
# one of sampling, eps_greedy, entropybased
policy_type: "eps_greedy"
rescale_dims: 84

# experiment settings
seed: 23
cuda: yes
display_plots: yes # uses vizdom for plotting

# training settings
training_steps: 1000000
done_after_lost_life: yes

# estimator settings
estimator: atari
hidden_size: 128
batch_size: 32
hist_len: 4

# exploration, q-learning, optimization settings
epsilon: 1
epsilon_steps: 1000
experience_replay: nTupleExperienceReplay
replay_mem_size: 10000
start_learning_after: 1000
cache: 16 # no of batches to cache
update_freq: 4
target_update_freq: 10000
gamma: 0.99
optim: RMSprop
lr: .00025  # decrease to 0.000135 for hist_len 4
eps: 0.01  # RMSprop: 0.01 | Adam: 0.01/batch_size (categorical)
alpha: 0.95  # RMSprop smoothing constant
# clamps
reward_clamp: yes # [-1, 1]

# support settings
atoms_no: 51
v_min: -10
v_max: 10

# evaluator
eval_steps: 2000
eval_frequency: 10000
eval_start: 5000
eval_env_name: "Breakout-v0"  # eg.: deterministic or wo skipping frames

# reporting
report_frequency: 10000 # steps
